import requests
from bs4 import BeautifulSoup
import csv

csv_header_part2 = ['Product URL', 'Description', 'ASIN', 'Product Description', 'Manufacturer']
csv_rows_part2 = []

def scrape_additional_products_info(page_url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'
    }
    response = requests.get(page_url, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")

    # Get description
    try:
        description = ". ".join([x.text.strip() for x in soup.find('div', {'id': 'feature-bullets'}).find_all('span')])
    except:
        description = "NA"

    # Get product description
    try:
        product_description = soup.find('div', {'id': 'productDescription'}).find('span').text
    except:
        product_description = "NA"

    # Get ASIN
    try:
        asin = soup.find('th', class_='a-color-secondary a-size-base prodDetSectionEntry', string=' ASIN ').find_next('td', class_='a-size-base prodDetAttrValue').text.strip()
    except:
        try:
            asin = soup.select('ul.a-unordered-list.a-nostyle.a-vertical.a-spacing-none.detail-bullet-list > li > span > span:nth-child(2)')[3].text
        except:
            asin = 'NA'

    # Get Manufacturer
    try:
        s = soup.find('table', class_="a-keyvalue prodDetTable")
        manufacturer = s.find('th', class_='a-color-secondary', string=' Manufacturer ').find_next('td', class_='a-size-base prodDetAttrValue').text.strip()
    except:
        try:
            manufacturer = soup.select('ul.a-unordered-list.a-nostyle.a-vertical.a-spacing-none.detail-bullet-list > li > span > span:nth-child(2)')[2].text
        except:
            manufacturer = "NA"

    return description, asin, product_description, manufacturer

# Load product URLs from the CSV file generated by part1.py
with open('amazon_products.csv', 'r', encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile)
    next(csvreader)  # Skip the header row
    for row in csvreader:
        product_url = row[0]

        # Scrape additional product information
        description, asin, product_description, manufacturer = scrape_additional_products_info(product_url)

        csv_rows_part2.append([product_url, description, asin, product_description, manufacturer])

# Save the data to a new CSV file
with open('amazon_products_part2.csv', 'w', encoding='utf-8', newline='') as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(csv_header_part2)
    csvwriter.writerows(csv_rows_part2)
